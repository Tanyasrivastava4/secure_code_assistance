# server/config.py

# Path to downloaded CodeLLaMA-7B weights
#MODEL_PATH = "server/models/codellama-7b"

# FastAPI host and port
#HOST = "0.0.0.0"
#PORT = 8080

# Max tokens to generate in response
#MAX_TOKENS = 1024

# Use float16 for faster GPU inference
#USE_FP16 = True
